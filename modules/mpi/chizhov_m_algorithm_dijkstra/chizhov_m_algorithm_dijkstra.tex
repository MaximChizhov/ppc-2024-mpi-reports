\documentclass[a4paper, 14pt]{extarticle}

% Поддержка языков
\usepackage[english, russian]{babel}

% Настройка отступов от краев страницы
\usepackage[left=3cm, right=1.5cm, top=2cm, bottom=2cm]{geometry}

\usepackage{titleps} % Колонтитулы
\usepackage{subfig} % Для подписей к рисункам и таблицам
% Пакет для отрисовки графиков
\usepackage{tikz}
\usetikzlibrary{arrows,positioning,shadows}
\usepackage{stmaryrd} % Стрелки в формулах
\usepackage{indentfirst} % Красная строка после заголовка
\usepackage{hhline} % Улучшенные горизонтальные линии в таблицах
\usepackage{multirow} % Ячейки в несколько строчек в таблицах
\usepackage{longtable} % Многостраничные таблицы
\usepackage{paralist,array} % Список внутри таблицы
\usepackage[normalem]{ulem}  % Зачеркнутый текст
\usepackage{upgreek, tipa} % Красивые греческие буквы
\usepackage{amsmath, amsfonts, amssymb, amsthm, mathtools} % ams пакеты для математики, табуляции
\usepackage{nicematrix} % Особые матрицы pNiceArray

\linespread{1.5} % Межстрочный интервал
\setlength{\parindent}{1.25cm} % Табуляция
\setlength{\parskip}{0cm}

% Пакет для красивого выделения кода
\usepackage{minted}
\setminted{fontsize=\footnotesize}

% Добавляем гипертекстовое оглавление в PDF
\usepackage[
bookmarks=true, colorlinks=true, unicode=true,
urlcolor=black,linkcolor=black, anchorcolor=black,
citecolor=black, menucolor=black, filecolor=black,
]{hyperref}

% Убрать переносы слов
\tolerance=1
\emergencystretch=\maxdimen
\hyphenpenalty=10000
\hbadness=10000

\newpagestyle{main}{
	% Верхний колонтитул
	\setheadrule{0cm} % Размер линии отделяющей колонтитул от страницы
	\sethead{}{}{} % Содержание {слева}{по центру}{справа}
	% Нижний колонтитул
	\setfootrule{0cm} % Размер линии отделяющей колонтитул от страницы
	\setfoot{}{}{\thepage} % Содержание {слева}{по центру}{справа}
}
\pagestyle{main}

% Выделение кода C++
\usepackage{listings}
\usepackage{xcolor} % для цветовой подсветки синтаксиса (не обязательно)
\lstset{
    language=C++, % язык программирования
    basicstyle=\ttfamily\small, % шрифт и размер текста
    keywordstyle=\color{blue}, % стиль для ключевых слов
    commentstyle=\color{green}, % стиль для комментариев
    stringstyle=\color{red}, % стиль для строк
    breaklines=true, % перенос строк
    numbers=left, % нумерация строк
    numberstyle=\tiny\color{gray}, % стиль для номеров строк
    frame=single, % рамка вокруг кода
    captionpos=b, % подпись снизу
}

% Гипперссылки
\usepackage{hyperref} % для автоматических ссылок
\addto\captionsrussian{
    \def\tablename{Таблица}
    \def\figurename{Рисунок}
    \def\refname{Список литературы}
}

\begin{document}
\begin{titlepage}

\begin{center}
Министерство науки и высшего образования Российской Федерации
\end{center}

\begin{center}
Федеральное государственное автономное образовательное учреждение высшего образования \\
Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского
\end{center}

\begin{center}
Институт информационных технологий, математики и механики
\end{center}

\vspace{4em}

\begin{center}
\textbf{\LargeОтчет по лабораторной работе} \\
\end{center}
\begin{center}
\textbf{\Large<<Поиск кратчайших путей из одной вершины (алгоритм Дейкстры). С CRS графов>>} \\
\end{center}

\vspace{4em}

\newbox{\lbox}
\savebox{\lbox}{\hbox{text}}
\newlength{\maxl}
\setlength{\maxl}{\wd\lbox}
\hfill\parbox{7cm}{
\hspace*{5cm}\hspace*{-5cm}\textbf{Выполнил:} \\ студент группы 3822Б1ФИ3\\Чижов М. А.\\
\\
\hspace*{5cm}\hspace*{-5cm}\textbf{Преподаватель:}\\ кандидат технических наук\\Сысоев А. В.\\
}
\vspace{\fill}

\begin{center} Нижний Новгород \\ 2024 \end{center}

\end{titlepage}

    \tableofcontents

    \newpage
    \section{Введение}
В современном мире графы играют ключевую роль в различных областях, таких как компьютерные сети, транспортные системы, социальные сети и многие другие. Графы представляют собой математические структуры, состоящие из вершин (узлов) и рёбер (связей) между ними. Одной из наиболее распространённых задач, связанных с графами, является нахождение кратчайших путей между вершинами. Эта задача имеет важное значение для оптимизации маршрутов, минимизации затрат и повышения эффективности различных процессов.\\

Алгоритм Дейкстры, разработанный нидерландским учёным Эдсгером Дейкстрой в 1959 году, является одним из самых известных и широко используемых методов для решения задачи нахождения кратчайшего пути в графах с неотрицательными весами рёбер. Он позволяет находить кратчайшие пути от одной начальной вершины до всех остальных вершин графа, что делает его полезным в таких приложениях, как маршрутизация в компьютерных сетях, планирование логистики и навигация.
    \newpage
    \section{Цель работы}
Целью данной лабораторной работы является реализация алгоритма Дейкстры с CRS графов с использованием MPI.

Для достижения этой цели необходимо:  
\begin{itemize}
    \item Реализовать последовательную версию алгоритм Дейкстры.
    \item Реализовать параллельную версию алгоритма Дейкстры с использованием библиотеки MPI.
    \item Провести тестирование реализованных алгоритмов.
\end{itemize}  

    \newpage
    \section{Описание алгоритмов}

\subsection{Последовательный алгоритм}

Инициализируются векторы для хранения кратчайших расстояний и информации о посещенных вершинах. Алгоритм ищет вершину с минимальным расстоянием среди непосещенных. Если такая вершина не найдена, цикл прерывается. После нахождения минимальной вершины, она помечается как посещенная, и алгоритм обновляет расстояния для всех её соседей. Для каждой соседней вершины проверяется, можно ли улучшить расстояние через текущую вершину. Если да, то значение расстояния обновляется.

\subsection{Параллельный алгоритм}

Алгоритм определяет диапазон вершин, который будет обрабатываться каждым процессом, основываясь на количестве процессов и размере графа. Каждый процесс инициализирует векторы для хранения кратчайших расстояний и информации о посещенных вершинах. Начальная вершина устанавливается в 0 для процесса с нулевым рангом, после чего происходит распространение начальных расстояний для всех процессов.\\

В основном цикле каждый процесс ищет локальную минимальную непосещенную вершину. Для этого он перебирает вершины в своем диапазоне и обновляет локальные значения, представляющие минимальное расстояние и соответствующий индекс. Затем выполняется операция, которая позволяет всем процессам совместно определить минимальное расстояние и индекс среди всех. Если минимальное значение указывает на то, что все вершины были посещены, цикл прерывается.\\

После нахождения глобально минимальной вершины, она помечается как посещенная. Алгоритм обновляет расстояния для всех соседей этой вершины, проверяя, возможно ли улучшить расстояние до соседних вершин. Если это возможно, значение расстояния обновляется.

    \newpage
    \section{Экспериментальная часть}
В данной части работы описан процесс проведения экспериментов по измерению времени выполнения алгоритмов последовательной и параллельной реализаций.

\subsection{Последовательная реализация}

Функциональные тесты\\

\begin{lstlisting}
#include <gtest/gtest.h>

#include <vector>

#include "seq/chizhov_m_algorithm_dijkstra/include/ops_seq.hpp"

TEST(chizhov_m_dijkstra_realization, Test_Graph_3x3) {
  int size = 3;
  int st = 0;

  // Create data
  std::vector<int> matrix = {0, 2, 5, 4, 0, 2, 3, 1, 0};
  std::vector<int> res(size, 0);
  std::vector<int> ans = {0, 2, 4};

  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
  taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
  taskDataSeq->inputs_count.emplace_back(matrix.size());
  taskDataSeq->inputs_count.emplace_back(size);
  taskDataSeq->inputs_count.emplace_back(st);
  taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
  taskDataSeq->outputs_count.emplace_back(res.size());

  // Create Task
  chizhov_m_dijkstra_seq::TestTaskSequential testTaskSequential(taskDataSeq);
  ASSERT_EQ(testTaskSequential.validation(), true);
  testTaskSequential.pre_processing();
  testTaskSequential.run();
  testTaskSequential.post_processing();
  ASSERT_EQ(ans, res);
}

TEST(chizhov_m_dijkstra_realization_seq, Test_Graph_4x4) {
  int size = 4;
  int st = 0;

  // Create data
  std::vector<int> matrix = {0, 9, 9, 3, 6, 0, 3, 5, 1, 3, 0, 5, 2, 2, 10, 0};
  std::vector<int> res(size, 0);
  std::vector<int> ans = {0, 5, 8, 3};

  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
  taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
  taskDataSeq->inputs_count.emplace_back(matrix.size());
  taskDataSeq->inputs_count.emplace_back(size);
  taskDataSeq->inputs_count.emplace_back(st);
  taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
  taskDataSeq->outputs_count.emplace_back(res.size());

  // Create Task
  chizhov_m_dijkstra_seq::TestTaskSequential testTaskSequential(taskDataSeq);
  ASSERT_EQ(testTaskSequential.validation(), true);
  testTaskSequential.pre_processing();
  testTaskSequential.run();
  testTaskSequential.post_processing();
  ASSERT_EQ(ans, res);
}

TEST(chizhov_m_dijkstra_realization_seq, Test_Graph_5x5) {
  int size = 5;
  int st = 0;

  // Create data
  std::vector<int> matrix = {
      0, 5, 0, 3, 0, 0, 0, 4, 2, 2, 0, 0, 0, 3, 0, 0, 3, 0, 0, 2, 9, 0, 1, 0, 0
  };
  std::vector<int> res(size, 0);
  std::vector<int> ans = {0, 5, 6, 3, 5};

  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
  taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
  taskDataSeq->inputs_count.emplace_back(matrix.size());
  taskDataSeq->inputs_count.emplace_back(size);
  taskDataSeq->inputs_count.emplace_back(st);
  taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
  taskDataSeq->outputs_count.emplace_back(res.size());

  // Create Task
  chizhov_m_dijkstra_seq::TestTaskSequential testTaskSequential(taskDataSeq);
  ASSERT_EQ(testTaskSequential.validation(), true);
  testTaskSequential.pre_processing();
  testTaskSequential.run();
  testTaskSequential.post_processing();
  ASSERT_EQ(ans, res);
}

TEST(chizhov_m_dijkstra_realization_seq, Test_Negative_Value) {
  int size = 3;
  int st = 0;

  // Create data
  std::vector<int> matrix = {0, 2, 5, -4, 0, 2, 3, 1, 0};
  std::vector<int> res(size, 0);

  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
  taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
  taskDataSeq->inputs_count.emplace_back(matrix.size());
  taskDataSeq->inputs_count.emplace_back(size);
  taskDataSeq->inputs_count.emplace_back(st);
  taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
  taskDataSeq->outputs_count.emplace_back(res.size());

  // Create Task
  chizhov_m_dijkstra_seq::TestTaskSequential testTaskSequential(taskDataSeq);
  ASSERT_FALSE(testTaskSequential.validation());
}

TEST(chizhov_m_dijkstra_realization_seq, Test_Source_Vertex_False) {
  int size = 3;
  int st = 5;

  // Create data
  std::vector<int> matrix = {0, 2, 5, 4, 0, 2, 3, 1, 0};
  std::vector<int> res(size, 0);

  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
    taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
  taskDataSeq->inputs_count.emplace_back(matrix.size());
  taskDataSeq->inputs_count.emplace_back(size);
  taskDataSeq->inputs_count.emplace_back(st);
  taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
  taskDataSeq->outputs_count.emplace_back(res.size());

  // Create Task
  chizhov_m_dijkstra_seq::TestTaskSequential testTaskSequential(taskDataSeq);
  ASSERT_FALSE(testTaskSequential.validation());
}
\end{lstlisting}

Тесты на производительность\\

\begin{lstlisting}
    #include <gtest/gtest.h>

#include <vector>

#include "core/perf/include/perf.hpp"
#include "seq/chizhov_m_algorithm_dijkstra/include/ops_seq.hpp"

TEST(chizhov_m_dijkstra_seq_perf_test, test_pipeline_run) {
  // Create data
  int count_size_vector = 5000;
  int st = 0;
  std::vector<int> global_matrix(count_size_vector * count_size_vector, 1);
  std::vector<int32_t> global_path(count_size_vector, 1);

  for (int i = 0; i < count_size_vector; i++) {
    global_matrix[i * count_size_vector + i] = 0;
  }
  global_path[0] = 0;

  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
  taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(global_matrix.data()));
  taskDataSeq->inputs_count.emplace_back(global_matrix.size());
  taskDataSeq->inputs_count.emplace_back(count_size_vector);
  taskDataSeq->inputs_count.emplace_back(st);
  taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(global_path.data()));
  taskDataSeq->outputs_count.emplace_back(global_path.size());

  // Create Task
  auto testTaskSequential = std::make_shared<chizhov_m_dijkstra_seq::TestTaskSequential>(taskDataSeq);

  // Create Perf attributes
  auto perfAttr = std::make_shared<ppc::core::PerfAttr>();
  perfAttr->num_running = 10;
  const auto t0 = std::chrono::high_resolution_clock::now();
  perfAttr->current_timer = [&] {
    auto current_time_point = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::nanoseconds>(current_time_point - t0).count();
    return static_cast<double>(duration) * 1e-9;
  };

  // Create and init perf results
  auto perfResults = std::make_shared<ppc::core::PerfResults>();

  // Create Perf analyzer
  auto perfAnalyzer = std::make_shared<ppc::core::Perf>(testTaskSequential);
  perfAnalyzer->pipeline_run(perfAttr, perfResults);
  ppc::core::Perf::print_perf_statistic(perfResults);
  ASSERT_EQ(1, global_path[1]);
}

TEST(chizhov_m_dijkstra_seq_perf_test, test_task_run) {
  // Create data
  int count_size_vector = 5000;
  int st = 0;
  std::vector<int> global_matrix(count_size_vector * count_size_vector, 1);
  std::vector<int32_t> global_path(count_size_vector, 1);

  for (int i = 0; i < count_size_vector; i++) {
    global_matrix[i * count_size_vector + i] = 0;
  }
  global_path[0] = 0;

  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
  taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(global_matrix.data()));
  taskDataSeq->inputs_count.emplace_back(global_matrix.size());
  taskDataSeq->inputs_count.emplace_back(count_size_vector);
  taskDataSeq->inputs_count.emplace_back(st);
  taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(global_path.data()));
  taskDataSeq->outputs_count.emplace_back(global_path.size());

  // Create Task
  auto testTaskSequential = std::make_shared<chizhov_m_dijkstra_seq::TestTaskSequential>(taskDataSeq);

  // Create Perf attributes
  auto perfAttr = std::make_shared<ppc::core::PerfAttr>();
  perfAttr->num_running = 10;
  const auto t0 = std::chrono::high_resolution_clock::now();
  perfAttr->current_timer = [&] {
    auto current_time_point = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::nanoseconds>(current_time_point - t0).count();
    return static_cast<double>(duration) * 1e-9;
  };

  // Create and init perf results
  auto perfResults = std::make_shared<ppc::core::PerfResults>();

  // Create Perf analyzer
  auto perfAnalyzer = std::make_shared<ppc::core::Perf>(testTaskSequential);
  perfAnalyzer->task_run(perfAttr, perfResults);
  ppc::core::Perf::print_perf_statistic(perfResults);
  ASSERT_EQ(1, global_path[1]);
}
\end{lstlisting}

\subsection{Параллельная реализация}

Функциональные тесты\\

\begin{lstlisting}
    #include <gtest/gtest.h>

#include <boost/mpi/communicator.hpp>
#include <boost/mpi/environment.hpp>
#include <iomanip>
#include <random>
#include <vector>

#include "mpi/chizhov_m_algorithm_dijkstra/include/ops_mpi.hpp"
void chizhov_m_dijkstra_mpi::generateMatrix(std::vector<int> &w, int n, int min, int max) {
  std::random_device dev;
  std::mt19937 gen(dev());
  std::uniform_int_distribution<int> dist(min, max);
  for (int i = 0; i < n * n; i++) {
    int val = dist(gen);
    w[i] = val;
  }
  for (int i = 0; i < n; i++) {
    w[i * n + i] = 0;
  }
}

TEST(chizhov_m_dijkstra_realization_mpi, Test_Graph_5_vertex) {
  boost::mpi::communicator world;
  int size = 5;
  int st = 0;
  int min = 1;
  int max = 10;

  std::vector<int> matrix(size * size, 0);

  std::vector<int> res(size, 0);
  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataPar = std::make_shared<ppc::core::TaskData>();

  if (world.rank() == 0) {
    chizhov_m_dijkstra_mpi::generateMatrix(matrix, size, min, max);
    taskDataPar->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataPar->inputs_count.emplace_back(matrix.size());
    taskDataPar->inputs_count.emplace_back(size);
    taskDataPar->inputs_count.emplace_back(st);
    taskDataPar->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
    taskDataPar->outputs_count.emplace_back(res.size());
  }

  chizhov_m_dijkstra_mpi::TestMPITaskParallel testMpiTaskParallel(taskDataPar);
  ASSERT_EQ(testMpiTaskParallel.validation(), true);
  testMpiTaskParallel.pre_processing();
  testMpiTaskParallel.run();
  testMpiTaskParallel.post_processing();

  if (world.rank() == 0) {
    // Create data
    std::vector<int> res_seq(size, 0);

    // Create TaskData
    std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
    taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataSeq->inputs_count.emplace_back(matrix.size());
    taskDataSeq->inputs_count.emplace_back(size);
    taskDataSeq->inputs_count.emplace_back(st);
    taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(res_seq.data()));
    taskDataSeq->outputs_count.emplace_back(res_seq.size());

    // Create Task
    chizhov_m_dijkstra_mpi::TestMPITaskSequential testMpiTaskSequential(taskDataSeq);
    ASSERT_EQ(testMpiTaskSequential.validation(), true);
    testMpiTaskSequential.pre_processing();
    testMpiTaskSequential.run();
    testMpiTaskSequential.post_processing();

    ASSERT_EQ(res_seq, res);
  }
}

TEST(chizhov_m_dijkstra_realization_mpi, Test_Graph_10_vertex) {
  boost::mpi::communicator world;
  int size = 10;
  int st = 3;
  int min = 5;
  int max = 50;

  std::vector<int> matrix(size * size, 0);
  std::vector<int> res(size, 0);
  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataPar = std::make_shared<ppc::core::TaskData>();

  if (world.rank() == 0) {
    chizhov_m_dijkstra_mpi::generateMatrix(matrix, size, min, max);
    taskDataPar->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataPar->inputs_count.emplace_back(matrix.size());
    taskDataPar->inputs_count.emplace_back(size);
    taskDataPar->inputs_count.emplace_back(st);
    taskDataPar->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
    taskDataPar->outputs_count.emplace_back(res.size());
  }

  chizhov_m_dijkstra_mpi::TestMPITaskParallel testMpiTaskParallel(taskDataPar);
  ASSERT_EQ(testMpiTaskParallel.validation(), true);
  testMpiTaskParallel.pre_processing();
  testMpiTaskParallel.run();
  testMpiTaskParallel.post_processing();

  if (world.rank() == 0) {
    // Create data
    std::vector<int> res_seq(size, 0);

    // Create TaskData
    std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
    taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataSeq->inputs_count.emplace_back(matrix.size());
    taskDataSeq->inputs_count.emplace_back(size);
    taskDataSeq->inputs_count.emplace_back(st);
    taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(res_seq.data()));
    taskDataSeq->outputs_count.emplace_back(res_seq.size());

    // Create Task
    chizhov_m_dijkstra_mpi::TestMPITaskSequential testMpiTaskSequential(taskDataSeq);
    ASSERT_EQ(testMpiTaskSequential.validation(), true);
    testMpiTaskSequential.pre_processing();
    testMpiTaskSequential.run();
    testMpiTaskSequential.post_processing();

    ASSERT_EQ(res_seq, res);
  }
}

TEST(chizhov_m_dijkstra_realization_mpi, Test_Graph_13_vertex) {
  boost::mpi::communicator world;
  int size = 13;
  int st = 3;
  int min = 4;
  int max = 20;

  std::vector<int> matrix(size * size, 0);
  std::vector<int> res(size, 0);
  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataPar = std::make_shared<ppc::core::TaskData>();

  if (world.rank() == 0) {
    chizhov_m_dijkstra_mpi::generateMatrix(matrix, size, min, max);
    taskDataPar->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataPar->inputs_count.emplace_back(matrix.size());
    taskDataPar->inputs_count.emplace_back(size);
    taskDataPar->inputs_count.emplace_back(st);
    taskDataPar->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
    taskDataPar->outputs_count.emplace_back(res.size());
  }

  chizhov_m_dijkstra_mpi::TestMPITaskParallel testMpiTaskParallel(taskDataPar);
  ASSERT_EQ(testMpiTaskParallel.validation(), true);
  testMpiTaskParallel.pre_processing();
  testMpiTaskParallel.run();
  testMpiTaskParallel.post_processing();

  if (world.rank() == 0) {
    // Create data
    std::vector<int> res_seq(size, 0);

    // Create TaskData
    std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
    taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataSeq->inputs_count.emplace_back(matrix.size());
    taskDataSeq->inputs_count.emplace_back(size);
    taskDataSeq->inputs_count.emplace_back(st);
    taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(res_seq.data()));
    taskDataSeq->outputs_count.emplace_back(res_seq.size());

    // Create Task
    chizhov_m_dijkstra_mpi::TestMPITaskSequential testMpiTaskSequential(taskDataSeq);
    ASSERT_EQ(testMpiTaskSequential.validation(), true);
    testMpiTaskSequential.pre_processing();
    testMpiTaskSequential.run();
    testMpiTaskSequential.post_processing();

    ASSERT_EQ(res_seq, res);
  }
}

TEST(chizhov_m_dijkstra_realization_mpi, Test_Graph_20_vertex) {
  boost::mpi::communicator world;
  int size = 20;
  int st = 3;
  int min = 2;
  int max = 40;

  std::vector<int> matrix(size * size, 0);
  std::vector<int> res(size, 0);
  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataPar = std::make_shared<ppc::core::TaskData>();

  if (world.rank() == 0) {
    chizhov_m_dijkstra_mpi::generateMatrix(matrix, size, min, max);
    taskDataPar->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataPar->inputs_count.emplace_back(matrix.size());
    taskDataPar->inputs_count.emplace_back(size);
    taskDataPar->inputs_count.emplace_back(st);
    taskDataPar->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
    taskDataPar->outputs_count.emplace_back(res.size());
  }

  chizhov_m_dijkstra_mpi::TestMPITaskParallel testMpiTaskParallel(taskDataPar);
  ASSERT_EQ(testMpiTaskParallel.validation(), true);
  testMpiTaskParallel.pre_processing();
  testMpiTaskParallel.run();
  testMpiTaskParallel.post_processing();

  if (world.rank() == 0) {
    // Create data
    std::vector<int> res_seq(size, 0);

    // Create TaskData
    std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
    taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataSeq->inputs_count.emplace_back(matrix.size());
    taskDataSeq->inputs_count.emplace_back(size);
    taskDataSeq->inputs_count.emplace_back(st);
    taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(res_seq.data()));
    taskDataSeq->outputs_count.emplace_back(res_seq.size());

    // Create Task
    chizhov_m_dijkstra_mpi::TestMPITaskSequential testMpiTaskSequential(taskDataSeq);
    ASSERT_EQ(testMpiTaskSequential.validation(), true);
    testMpiTaskSequential.pre_processing();
    testMpiTaskSequential.run();
    testMpiTaskSequential.post_processing();

    ASSERT_EQ(res_seq, res);
  }
}

TEST(chizhov_m_dijkstra_realization_mpi, Test_Source_Vertex_False) {
  boost::mpi::communicator world;
  int size = 10;
  int st = 13;
  int min = 2;
  int max = 20;

  std::vector<int> matrix(size * size, 0);
  std::vector<int> res(size, 0);
  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataPar = std::make_shared<ppc::core::TaskData>();

  if (world.rank() == 0) {
    chizhov_m_dijkstra_mpi::generateMatrix(matrix, size, min, max);
    taskDataPar->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataPar->inputs_count.emplace_back(matrix.size());
    taskDataPar->inputs_count.emplace_back(size);
    taskDataPar->inputs_count.emplace_back(st);
    taskDataPar->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
    taskDataPar->outputs_count.emplace_back(res.size());
  }

  chizhov_m_dijkstra_mpi::TestMPITaskParallel testMpiTaskParallel(taskDataPar);
  if (world.rank() == 0) {
    ASSERT_FALSE(testMpiTaskParallel.validation());
  }
}

TEST(chizhov_m_dijkstra_realization_mpi, Test_Negative_Value) {
  boost::mpi::communicator world;
  int size = 3;
  int st = 0;
  
  std::vector<int> matrix = {0, 2, 5, 4, 0, 2, 3, -1, 0};
  std::vector<int> res(size, 0);
  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataPar = std::make_shared<ppc::core::TaskData>();

  if (world.rank() == 0) {
    taskDataPar->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataPar->inputs_count.emplace_back(matrix.size());
    taskDataPar->inputs_count.emplace_back(size);
    taskDataPar->inputs_count.emplace_back(st);
    taskDataPar->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
    taskDataPar->outputs_count.emplace_back(res.size());
  }

  chizhov_m_dijkstra_mpi::TestMPITaskParallel testMpiTaskParallel(taskDataPar);
  if (world.rank() == 0) {
    ASSERT_FALSE(testMpiTaskParallel.validation());
  }
}

TEST(chizhov_m_dijkstra_realization_mpi, Test_Spare_Graph_5x5) {
  boost::mpi::communicator world;
  int size = 5;
  int st = 0;

  // Create data
  std::vector<int> matrix = {0, 5, 0, 3, 0, 0, 0, 4, 2, 2, 0, 0, 0, 3, 0, 0, 3, 0, 0, 2, 9, 0, 1, 0, 0};
  std::vector<int> res(size, 0);
  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataPar = std::make_shared<ppc::core::TaskData>();

  if (world.rank() == 0) {
    taskDataPar->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataPar->inputs_count.emplace_back(matrix.size());
    taskDataPar->inputs_count.emplace_back(size);
    taskDataPar->inputs_count.emplace_back(st);
    taskDataPar->outputs.emplace_back(reinterpret_cast<uint8_t *>(res.data()));
    taskDataPar->outputs_count.emplace_back(res.size());
  }

  chizhov_m_dijkstra_mpi::TestMPITaskParallel testMpiTaskParallel(taskDataPar);
  ASSERT_EQ(testMpiTaskParallel.validation(), true);
  testMpiTaskParallel.pre_processing();
  testMpiTaskParallel.run();
  testMpiTaskParallel.post_processing();

  if (world.rank() == 0) {
    // Create data
    std::vector<int> res_seq(size, 0);

    // Create TaskData
    std::shared_ptr<ppc::core::TaskData> taskDataSeq = std::make_shared<ppc::core::TaskData>();
    taskDataSeq->inputs.emplace_back(reinterpret_cast<uint8_t *>(matrix.data()));
    taskDataSeq->inputs_count.emplace_back(matrix.size());
    taskDataSeq->inputs_count.emplace_back(size);
    taskDataSeq->inputs_count.emplace_back(st);
    taskDataSeq->outputs.emplace_back(reinterpret_cast<uint8_t *>(res_seq.data()));
    taskDataSeq->outputs_count.emplace_back(res_seq.size());

    // Create Task
    chizhov_m_dijkstra_mpi::TestMPITaskSequential testMpiTaskSequential(taskDataSeq);
    ASSERT_EQ(testMpiTaskSequential.validation(), true);
    testMpiTaskSequential.pre_processing();
    testMpiTaskSequential.run();
    testMpiTaskSequential.post_processing();

    ASSERT_EQ(res_seq, res);
  }
}
\end{lstlisting}

Тесты на производительность\\

\begin{lstlisting}
    #include <gtest/gtest.h>

#include <boost/mpi/timer.hpp>
#include <vector>

#include "core/perf/include/perf.hpp"
#include "mpi/chizhov_m_algorithm_dijkstra/include/ops_mpi.hpp"

TEST(chizhov_m_dijkstra_mpi_perf_test, test_pipeline_run) {
  boost::mpi::communicator world;
  int count_size_vector = 1000;
  int st = 0;
  std::vector<int> global_matrix(count_size_vector * count_size_vector, 3);
  std::vector<int32_t> global_path(count_size_vector, 3);
  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataPar = std::make_shared<ppc::core::TaskData>();
  if (world.rank() == 0) {
    for (int i = 0; i < count_size_vector; i++) {
      global_matrix[i * count_size_vector + i] = 0;
    }
    global_path[0] = 0;
    taskDataPar->inputs.emplace_back(reinterpret_cast<uint8_t *>(global_matrix.data()));
    taskDataPar->inputs_count.emplace_back(global_matrix.size());
    taskDataPar->inputs_count.emplace_back(count_size_vector);
    taskDataPar->inputs_count.emplace_back(st);
    taskDataPar->outputs.emplace_back(reinterpret_cast<uint8_t *>(global_path.data()));
    taskDataPar->outputs_count.emplace_back(global_path.size());
  }

  auto testMpiTaskParallel = std::make_shared<chizhov_m_dijkstra_mpi::TestMPITaskParallel>(taskDataPar);
  ASSERT_EQ(testMpiTaskParallel->validation(), true);
  testMpiTaskParallel->pre_processing();
  testMpiTaskParallel->run();
  testMpiTaskParallel->post_processing();

  // Create Perf attributes
  auto perfAttr = std::make_shared<ppc::core::PerfAttr>();
  perfAttr->num_running = 10;
  const boost::mpi::timer current_timer;
  perfAttr->current_timer = [&] { return current_timer.elapsed(); };

  // Create and init perf results
  auto perfResults = std::make_shared<ppc::core::PerfResults>();

  // Create Perf analyzer
  auto perfAnalyzer = std::make_shared<ppc::core::Perf>(testMpiTaskParallel);
  perfAnalyzer->pipeline_run(perfAttr, perfResults);
  if (world.rank() == 0) {
    ppc::core::Perf::print_perf_statistic(perfResults);
    ASSERT_EQ(3, global_path[3]);
  }
}

TEST(chizhov_m_dijkstra_mpi_perf_test, test_task_run) {
  boost::mpi::communicator world;
  int count_size_vector = 1000;
  int st = 5;
  std::vector<int> global_matrix(count_size_vector * count_size_vector, 3);
  std::vector<int32_t> global_path(count_size_vector, 3);
  // Create TaskData
  std::shared_ptr<ppc::core::TaskData> taskDataPar = std::make_shared<ppc::core::TaskData>();
  if (world.rank() == 0) {
    for (int i = 0; i < count_size_vector; i++) {
      global_matrix[i * count_size_vector + i] = 0;
    }
    global_path[0] = 0;
    taskDataPar->inputs.emplace_back(reinterpret_cast<uint8_t *>(global_matrix.data()));
    taskDataPar->inputs_count.emplace_back(global_matrix.size());
    taskDataPar->inputs_count.emplace_back(count_size_vector);
    taskDataPar->inputs_count.emplace_back(st);
    taskDataPar->outputs.emplace_back(reinterpret_cast<uint8_t *>(global_path.data()));
    taskDataPar->outputs_count.emplace_back(global_path.size());
  }

  auto testMpiTaskParallel = std::make_shared<chizhov_m_dijkstra_mpi::TestMPITaskParallel>(taskDataPar);
  ASSERT_EQ(testMpiTaskParallel->validation(), true);
  testMpiTaskParallel->pre_processing();
  testMpiTaskParallel->run();
  testMpiTaskParallel->post_processing();

  // Create Perf attributes
  auto perfAttr = std::make_shared<ppc::core::PerfAttr>();
  perfAttr->num_running = 10;
  const boost::mpi::timer current_timer;
  perfAttr->current_timer = [&] { return current_timer.elapsed(); };

  // Create and init perf results
  auto perfResults = std::make_shared<ppc::core::PerfResults>();

  // Create Perf analyzer
  auto perfAnalyzer = std::make_shared<ppc::core::Perf>(testMpiTaskParallel);
  perfAnalyzer->task_run(perfAttr, perfResults);
  if (world.rank() == 0) {
    ppc::core::Perf::print_perf_statistic(perfResults);
    ASSERT_EQ(3, global_path[3]);
  }
}
\end{lstlisting}
\newpage
\section{Заключение}

В ходе выполнения лабораторной работы была изучена реализация алгоритма Дейкстры для нахождения кратчайших путей в графах, а также проведено сравнение его параллельной и последовательной версий. Алгоритм Дейкстры, как один из наиболее известных методов для решения задачи кратчайшего пути, продемонстрировал свою эффективность в различных сценариях, включая работу с разреженными графами.\\

В процессе работы была реализована генерация графов, что позволило протестировать алгоритм на различных входных данных. Замеры времени выполнения показали, что параллельная версия алгоритма может значительно ускорить процесс нахождения кратчайших путей по сравнению с последовательной, особенно на больших графах. Это подтверждает актуальность и важность использования параллельных вычислений в задачах, связанных с обработкой больших объёмов данных.\\

Также была рассмотрена структура данных, используемая для представления графа, а именно формат CRS (Compressed Row Storage), который позволяет эффективно хранить и обрабатывать разреженные матрицы. Это решение оптимизирует использование памяти и ускоряет доступ к элементам графа.

\newpage
\section{Литература}
\begin{enumerate}
    \item «C/C++ в задачах  примерах», Никита Культин.
    \item «Кратчайшие пути в графах. BFS. Dijkstra.».
    \url {https://algorithmica.org/tg/bfs}.
\end{enumerate}
\newpage
\section{Приложения}
\subsubsection{Параллельная версия. Объявление}
\begin{lstlisting}
#pragma once

#include <gtest/gtest.h>

#include <boost/mpi/collectives.hpp>
#include <boost/mpi/communicator.hpp>
#include <memory>
#include <vector>

#include "core/task/include/task.hpp"

namespace chizhov_m_dijkstra_mpi {

void generateMatrix(std::vector<int>& w, int n, int min, int max);

void convertToCRS(const std::vector<int>& w, std::vector<int>& values, std::vector<int>& colIndex,
                  std::vector<int>& rowPtr, int n);

class TestMPITaskSequential : public ppc::core::Task {
 public:
  explicit TestMPITaskSequential(std::shared_ptr<ppc::core::TaskData> taskData_) : Task(std::move(taskData_)) {}
  bool pre_processing() override;
  bool validation() override;
  bool run() override;
  bool post_processing() override;

 private:
  std::vector<int> input_;
  std::vector<int> res_;
  int st{};
  int size{};
};

class TestMPITaskParallel : public ppc::core::Task {
 public:
  explicit TestMPITaskParallel(std::shared_ptr<ppc::core::TaskData> taskData_) : Task(std::move(taskData_)) {}
  bool pre_processing() override;
  bool validation() override;
  bool run() override;
  bool post_processing() override;

 private:
  std::vector<int> input_;
  std::vector<int> res_;
  std::vector<int> values;
  std::vector<int> colIndex;
  std::vector<int> rowPtr;
  int st{};
  int size{};
  boost::mpi::communicator world;
};

}  // namespace chizhov_m_dijkstra_mpib = tmp;
}
\end{lstlisting}

\newpage
\subsubsection{Параллельная версия. Реализация}
\begin{lstlisting}
// Copyright 2023 Nesterov Alexander
#include "mpi/chizhov_m_algorithm_dijkstra/include/ops_mpi.hpp"

#include <algorithm>
#include <functional>
#include <vector>

const int INF = std::numeric_limits<int>::max();

void chizhov_m_dijkstra_mpi::convertToCRS(const std::vector<int>& w, std::vector<int>& values,
                                          std::vector<int>& colIndex, std::vector<int>& rowPtr, int n) {
  rowPtr.resize(n + 1);
  int nnz = 0;
  for (int i = 0; i < n; i++) {
    rowPtr[i] = nnz;
    for (int j = 0; j < n; j++) {
      int weight = w[i * n + j];
      if (weight != 0) {
        values.emplace_back(weight);
        colIndex.emplace_back(j);
        nnz++;
      }
    }
  }
  rowPtr[n] = nnz;
}

bool chizhov_m_dijkstra_mpi::TestMPITaskSequential::pre_processing() {
  internal_order_test();
  // Init vectors
  size = taskData->inputs_count[1];
  st = taskData->inputs_count[2];

  input_ = std::vector<int>(size * size);
  auto* tmp_ptr = reinterpret_cast<int*>(taskData->inputs[0]);
  input_.assign(tmp_ptr, tmp_ptr + taskData->inputs_count[0]);

  res_ = std::vector<int>(size, 0);
  return true;
}

bool chizhov_m_dijkstra_mpi::TestMPITaskSequential::validation() {
  internal_order_test();

  if (taskData->inputs.empty()) {
    return false;
  }

  if (taskData->inputs_count.size() < 2 || taskData->inputs_count[1] <= 1) {
    return false;
  }

  auto* tmp_ptr = reinterpret_cast<int*>(taskData->inputs[0]);
  if (!std::all_of(tmp_ptr, tmp_ptr + taskData->inputs_count[0], [](int val) { return val >= 0; })) {
    return false;
  }

  if (taskData->inputs_count[2] < 0 || taskData->inputs_count[2] >= taskData->inputs_count[1]) {
    return false;
  }

  if (taskData->outputs.empty() || taskData->outputs[0] == nullptr || taskData->outputs.size() != 1 ||
      taskData->outputs_count[0] != taskData->inputs_count[1]) {
    return false;
  }
  return true;
}

bool chizhov_m_dijkstra_mpi::TestMPITaskSequential::run() {
  internal_order_test();
  std::vector<int> values;
  std::vector<int> colIndex;
  std::vector<int> rowPtr;
  convertToCRS(input_, values, colIndex, rowPtr, size);

  std::vector<bool> visited(size, false);
  std::vector<int> D(size, INF);
  D[st] = 0;

  for (int i = 0; i < size; i++) {
    int min = INF;
    int index = -1;
    for (int j = 0; j < size; j++) {
      if (!visited[j] && D[j] < min) {
        min = D[j];
        index = j;
      }
    }

    if (index == -1) break;

    int u = index;
    visited[u] = true;

    for (int j = rowPtr[u]; j < rowPtr[u + 1]; j++) {
      int v = colIndex[j];
      int weight = values[j];

      if (!visited[v] && D[u] != INF && (D[u] + weight < D[v])) {
        D[v] = D[u] + weight;
      }
    }
  }

  res_ = D;

  return true;
}

bool chizhov_m_dijkstra_mpi::TestMPITaskSequential::post_processing() {
  internal_order_test();
  std::copy(res_.begin(), res_.end(), reinterpret_cast<int*>(taskData->outputs[0]));
  return true;
}

bool chizhov_m_dijkstra_mpi::TestMPITaskParallel::pre_processing() {
  internal_order_test();
  if (world.rank() == 0) {
    size = taskData->inputs_count[1];
    st = taskData->inputs_count[2];
  }

  if (world.rank() == 0) {
    input_ = std::vector<int>(size * size);
    auto* tmp_ptr = reinterpret_cast<int*>(taskData->inputs[0]);
    input_.assign(tmp_ptr, tmp_ptr + taskData->inputs_count[0]);
    convertToCRS(input_, values, colIndex, rowPtr, size);
  } else {
    input_ = std::vector<int>(size * size, 0);
  }
  return true;
}

bool chizhov_m_dijkstra_mpi::TestMPITaskParallel::validation() {
  internal_order_test();
  if (world.rank() == 0) {
    if (taskData->inputs.empty()) {
      return false;
    }

    if (taskData->inputs_count.size() < 2 || taskData->inputs_count[1] <= 1) {
      return false;
    }

    auto* tmp_ptr = reinterpret_cast<int*>(taskData->inputs[0]);
    if (!std::all_of(tmp_ptr, tmp_ptr + taskData->inputs_count[0], [](int val) { return val >= 0; })) {
      return false;
    }

    if (taskData->inputs_count[2] < 0 || taskData->inputs_count[2] >= taskData->inputs_count[1]) {
      return false;
    }

    if (taskData->outputs.empty() || taskData->outputs[0] == nullptr || taskData->outputs.size() != 1 ||
        taskData->outputs_count[0] != taskData->inputs_count[1]) {
      return false;
    }
  }
  return true;
}

bool chizhov_m_dijkstra_mpi::TestMPITaskParallel::run() {
  internal_order_test();

  boost::mpi::broadcast(world, size, 0);
  boost::mpi::broadcast(world, st, 0);

  // broadcast of CRS vectors
  int values_size = values.size();
  int rowPtr_size = rowPtr.size();
  int colIndex_size = colIndex.size();

  boost::mpi::broadcast(world, values_size, 0);
  boost::mpi::broadcast(world, rowPtr_size, 0);
  boost::mpi::broadcast(world, colIndex_size, 0);

  values.resize(values_size);
  rowPtr.resize(rowPtr_size);
  colIndex.resize(colIndex_size);

  boost::mpi::broadcast(world, values.data(), values.size(), 0);
  boost::mpi::broadcast(world, rowPtr.data(), rowPtr.size(), 0);
  boost::mpi::broadcast(world, colIndex.data(), colIndex.size(), 0);

  int delta = size / world.size();
  int extra = size % world.size();
  if (extra != 0) {
    delta += 1;
  }
  int start_index = world.rank() * delta;
  int end_index = std::min(size, delta * (world.rank() + 1));

  res_.resize(size, INT_MAX);
  std::vector<bool> visited(size, false);
  std::vector<int> D(size, INF);

  if (world.rank() == 0) {
    res_[st] = 0;
  }

  boost::mpi::broadcast(world, res_.data(), size, 0);

  for (int k = 0; k < size; k++) {
    int local_min = INF;
    int local_index = -1;

    for (int i = start_index; i < end_index; i++) {
      if (!visited[i] && res_[i] < local_min) {
        local_min = res_[i];
        local_index = i;
      }
    }

    std::pair<int, int> local_pair = {local_min, local_index};
    std::pair<int, int> global_pair = {INF, -1};

    boost::mpi::all_reduce(world, local_pair, global_pair,
                           [](const std::pair<int, int>& a, const std::pair<int, int>& b) {
                             if (a.first < b.first) return a;
                             if (a.first > b.first) return b;
                             return a;
                           });

    if (global_pair.first == INF || global_pair.second == -1) {
      break;
    }

    visited[global_pair.second] = true;

    for (int j = rowPtr[global_pair.second]; j < rowPtr[global_pair.second + 1]; j++) {
      int v = colIndex[j];
      int w = values[j];

      if (!visited[v] && res_[global_pair.second] != INF && (res_[global_pair.second] + w < res_[v])) {
        res_[v] = res_[global_pair.second] + w;
      }
    }

    boost::mpi::all_reduce(world, res_.data(), size, D.data(), boost::mpi::minimum<int>());
    res_ = D;
  }

  return true;
}

bool chizhov_m_dijkstra_mpi::TestMPITaskParallel::post_processing() {
  internal_order_test();
  if (world.rank() == 0) {
    std::copy(res_.begin(), res_.end(), reinterpret_cast<int*>(taskData->outputs[0]));
  }
  return true;
}
\end{lstlisting}

\newpage
\subsubsection{Последовательная версия. Объявление}
\begin{lstlisting}
// Copyright 2023 Nesterov Alexander
#pragma once

#include <vector>

#include "core/task/include/task.hpp"

namespace chizhov_m_dijkstra_seq {

void convertToCRS(const std::vector<int>& w, std::vector<int>& values, std::vector<int>& colIndex,
                  std::vector<int>& rowPtr, int n);

class TestTaskSequential : public ppc::core::Task {
 public:
  explicit TestTaskSequential(std::shared_ptr<ppc::core::TaskData> taskData_) : Task(std::move(taskData_)) {}
  bool pre_processing() override;
  bool validation() override;
  bool run() override;
  bool post_processing() override;

 private:
  std::vector<int> input_;
  std::vector<int> res_;
  int st{};
  int size{};
};

}  // namespace chizhov_m_dijkstra_seq
\end{lstlisting}

\newpage
\subsubsection{Последовательная версия. Реализация}
\begin{lstlisting}
// Copyright 2024 Nesterov Alexander
#include "seq/chizhov_m_algorithm_dijkstra/include/ops_seq.hpp"

#include <algorithm>
#include <vector>

const int INF = std::numeric_limits<int>::max();

bool chizhov_m_dijkstra_seq::TestTaskSequential::pre_processing() {
  internal_order_test();
  // Init value for input and output
  size = taskData->inputs_count[1];
  st = taskData->inputs_count[2];

  input_ = std::vector<int>(size * size);
  auto* tmp_ptr = reinterpret_cast<int*>(taskData->inputs[0]);
  input_.assign(tmp_ptr, tmp_ptr + taskData->inputs_count[0]);

  res_ = std::vector<int>(size, 0);
  return true;
}

bool chizhov_m_dijkstra_seq::TestTaskSequential::validation() {
  internal_order_test();

  if (taskData->inputs.empty()) {
    return false;
  }

  if (taskData->inputs_count.size() < 2 || taskData->inputs_count[1] <= 1) {
    return false;
  }

  auto* tmp_ptr = reinterpret_cast<int*>(taskData->inputs[0]);
  if (!std::all_of(tmp_ptr, tmp_ptr + taskData->inputs_count[0], [](int val) { return val >= 0; })) {
    return false;
  }

  if (taskData->inputs_count[2] < 0 || taskData->inputs_count[2] >= taskData->inputs_count[1]) {
    return false;
  }

  if (taskData->outputs.empty() || taskData->outputs[0] == nullptr || taskData->outputs.size() != 1 ||
      taskData->outputs_count[0] != taskData->inputs_count[1]) {
    return false;
  }

  return true;
}

void chizhov_m_dijkstra_seq::convertToCRS(const std::vector<int>& w, std::vector<int>& values,
                                          std::vector<int>& colIndex, std::vector<int>& rowPtr, int n) {
  rowPtr.resize(n + 1);
  int nnz = 0;

  for (int i = 0; i < n; i++) {
    rowPtr[i] = nnz;
    for (int j = 0; j < n; j++) {
      int weight = w[i * n + j];
      if (weight != 0) {
        values.emplace_back(weight);
        colIndex.emplace_back(j);
        nnz++;
      }
    }
  }
  rowPtr[n] = nnz;
}

bool chizhov_m_dijkstra_seq::TestTaskSequential::run() {
  internal_order_test();
  std::vector<int> values;
  std::vector<int> colIndex;
  std::vector<int> rowPtr;
  convertToCRS(input_, values, colIndex, rowPtr, size);

  std::vector<bool> visited(size, false);
  std::vector<int> D(size, INF);
  D[st] = 0;

  for (int i = 0; i < size; i++) {
    int min = INF;
    int index = -1;
    for (int j = 0; j < size; j++) {
      if (!visited[j] && D[j] < min) {
        min = D[j];
        index = j;
      }
    }

    if (index == -1) break;

    int u = index;
    visited[u] = true;

    for (int j = rowPtr[u]; j < rowPtr[u + 1]; j++) {
      int v = colIndex[j];
      int weight = values[j];

      if (!visited[v] && D[u] != INF && (D[u] + weight < D[v])) {
        D[v] = D[u] + weight;
      }
    }
  }

  res_ = D;

  return true;
}

bool chizhov_m_dijkstra_seq::TestTaskSequential::post_processing() {
  internal_order_test();
  std::copy(res_.begin(), res_.end(), reinterpret_cast<int*>(taskData->outputs[0]));
  return true;
}

\end{lstlisting}
	
\end{document}